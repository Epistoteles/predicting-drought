{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## US Drought & Meteorological Data Starter Notebook\n",
    "This notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Visualizing the Data\n",
    "In this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the csv files for training, validation and testing into the ``files`` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "sns.set_style('white')\n",
    "\n",
    "files = {}\n",
    "\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        if 'train' in filename:\n",
    "            files['train'] = os.path.join(dirname, filename)\n",
    "        if 'valid' in filename:\n",
    "            files['valid'] = os.path.join(dirname, filename)\n",
    "        if 'test' in filename:\n",
    "            files['test'] = os.path.join(dirname, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\n",
    "This could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n",
    "    for k in files.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one of 'train', 'valid' or 'test'\n",
    "def loadXY(df, shuffle=True, random_state=42, window_size=180, target_size=12):\n",
    "    soil_df = pd.read_csv('soil_data.csv')\n",
    "    time_data_cols = sorted([c for c in df.columns if c not in ['fips','date','score']])\n",
    "    static_data_cols = sorted([c for c in soil_df.columns if c not in ['soil','lat','lon']])\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=['score'])\n",
    "    X_static = np.empty((len(df)//window_size, len(static_data_cols)))\n",
    "    X_time = np.empty((len(df)//window_size, window_size, len(time_data_cols)))\n",
    "    y_past = np.empty((len(df)//window_size, window_size))\n",
    "    y_target = np.empty((len(df)//window_size, target_size))\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        start_i = np.random.randint(1, window_size)\n",
    "        fips_df = df[(df.index.get_level_values(0)==fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df['score'].values\n",
    "        X_s = soil_df[soil_df['fips']==fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y)-(window_size+target_size*7), window_size):\n",
    "            X_time[count] = X[i:i+window_size]\n",
    "            y_past[count] = interpolate_nans(y[i:i+window_size])\n",
    "            temp_y = y[i+window_size:i+window_size+target_size*7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    return X_static[:count], X_time[:count], y_past[:count], y_target[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_train, X_time_train, y_past_train, y_target_train = loadXY(dfs['train'], window_size=90, target_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_time_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sktime in /home/korbinian/.local/lib/python3.8/site-packages (0.6.1)\n",
      "Requirement already satisfied: wheel in /home/korbinian/.local/lib/python3.8/site-packages (from sktime) (0.36.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /home/korbinian/.local/lib/python3.8/site-packages (from sktime) (1.1.4)\n",
      "Requirement already satisfied: numba>=0.50 in /home/korbinian/.local/lib/python3.8/site-packages (from sktime) (0.53.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/korbinian/.local/lib/python3.8/site-packages (from sktime) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/korbinian/.local/lib/python3.8/site-packages (from sktime) (1.20.3)\n",
      "Requirement already satisfied: statsmodels>=0.12.1 in /home/korbinian/.local/lib/python3.8/site-packages (from sktime) (0.12.1)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/korbinian/.local/lib/python3.8/site-packages (from numba>=0.50->sktime) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /home/korbinian/.local/lib/python3.8/site-packages (from numba>=0.50->sktime) (56.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/korbinian/miniconda3/lib/python3.8/site-packages (from pandas>=1.1.0->sktime) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/korbinian/miniconda3/lib/python3.8/site-packages (from pandas>=1.1.0->sktime) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/korbinian/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->sktime) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/korbinian/.local/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/korbinian/miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/korbinian/miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.24.0->sktime) (1.0.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/korbinian/.local/lib/python3.8/site-packages (from statsmodels>=0.12.1->sktime) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sktime.datasets import load_arrow_head  # univariate dataset\n",
    "from sktime.datasets.base import load_basic_motions  # multivariate dataset\n",
    "from sktime.transformations.panel.rocket import MiniRocket, MiniRocketMultivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_time_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-862e5dd6941f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mminirocket_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniRocketMultivariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mminirocket_multi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_time_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminirocket_multi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_time_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_time_train' is not defined"
     ]
    }
   ],
   "source": [
    "minirocket_multi = MiniRocketMultivariate()\n",
    "minirocket_multi.fit(X_time_train)\n",
    "X_train_transform = minirocket_multi.transform(X_time_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "classifier.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_valid, X_time_valid, y_past_valid, y_target_valid = loadXY(dfs['valid'], window_size=90, target_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_transform = minirocket_multi.transform(X_time_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_valid_transform, y_target_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
